{"cells":[{"cell_type":"code","source":["# covid 19 tweet pre processing code\n","# code authors: rajdeep and jeevan\n","\n","from functools import partial\n","import pycountry\n","from pyspark.sql import SparkSession\n","import re\n","from pyspark.sql.functions import UserDefinedFunction\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import split, col, concat_ws\n","import numpy as np\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n","from pyspark.ml.feature import StopWordsRemover\n","\n","# set spark conf\n","spark.conf.set(\n","  \"\")\n","spark.conf.set(\n","  \"\")\n","\n","# remove unwanted values from text\n","def process_text (string):\n","    string = F.lower(string)\n","    string = F.regexp_replace(string, \"(https?\\://)\\S+\", \"\") # for links\n","    string = F.regexp_replace(string, \"(\\\\n)|\\n|\\r|\\t\", \"\") # for CR, tab, and LR\n","    string = F.regexp_replace(string, \"(?:(?:[0-9]{2}[:\\/,]){2}[0-9]{2,4})\", \"\") # for dates\n","    string = F.regexp_replace(string, \"@([A-Za-z0-9_]+)\", \"\") # for usernames\n","    string = F.regexp_replace(string, \"[0-9]\", \"\") # for numbers\n","    string = F.regexp_replace(string, \"\\:|\\/|\\#|\\.|\\?|\\!|\\&|\\\"|\\,\", \"\") # for symbols\n","    return string\n","\n","# match location of user\n","def add_true_location(user_location):\n","  for country in pycountry.countries:\n","    if country.name.upper() in user_location.upper():\n","      return country.name\n","  return ''\n","\n","# extract date\n","def to_date(array):\n","    date = array[1]+array[2]+\"2020\"\n","    return date  \n","\n","# udf\n","udf_func = F.udf(add_true_location)\n","udf_func2 = F.udf(to_date)\n","spark.udf.register(\"udf_func\", udf_func)\n","spark.udf.register(\"udf_func2\", udf_func2)  \n","\n","# read in stopwords txt file \n","stopWordsDf = spark.read.text(\"\")\n","# convert dff to array/list : https://mungingdata.com/pyspark/column-to-list-collect-tolocaliterator/\n","stopWords = [row[0] for row in stopWordsDf.select('value').collect()]\n","# remove stop words from \"full_text\" col and insert them into a new column \"filtered\"\n","remover = StopWordsRemover().setStopWords(stopWords).setInputCol(\"full_text\").setOutputCol(\"filtered\")\n","\n","\n","# configure which month and days to pre proccess\n","# month = feb, mar or apr\n","# month number = 2, 3, or 4\n","startDay = 1\n","endDay = 31\n","month = \"mar\"\n","monthNumber = \"3\"\n","\n","# for each day ...\n","for i in range(startDay, endDay+1):\n","  \n","  # read in file\n","  filename = 'geo_2020-0'+ monthNumber + '-' + str(i) + '.jsonl'\n","  df = spark.read.format(\"json\").load(\"wasbs://\"+month+\"@sentimentdatacs4225.blob.core.windows.net/\"+filename)\n","  \n","  df = df.select(\n","      col(\"created_at\"),\n","      col(\"full_text\"),\n","      col(\"lang\"),\n","      (col(\"user\")[\"location\"]).alias(\"user_location\")\n","  )\n","  \n","  # clean the text\n","  df = df.withColumn(\"full_text\", process_text(col(\"full_text\")))\n","\n","  # convert \"full_text\" column into array/list\n","  df = df.withColumn(\"full_text\", split(col(\"full_text\"), \" \"))\n","\n","  # find country from full_address \n","  df = df.withColumn('full_address', udf_func(df.user_location))\n","  df = df.filter(df.full_address != '')\n","\n","  # convert dates \n","  df = df.withColumn(\"created_at\", split(col(\"created_at\"), \"\\\\ \").alias(\"created_at\"))\n","  df = df.withColumn('created_at', udf_func2(df.created_at))\n","  df = df.withColumn('created_at', F.to_date(df.created_at, \"MMMddyyyy\"))\n","  \n","  # remove stopwords\n","  # array of string to string: https://sparkbyexamples.com/pyspark/pyspark-convert-array-column-to-string-column/ and drop \"full_text\" column\n","  df = remover.transform(df)\n","  df = df.withColumn(\"filtered\", concat_ws(\" \", col(\"filtered\")))\n","  df = df.drop(\"full_text\")\n","\n","  # select required fields, filter and write to file\n","  df = df.select('created_at', 'lang', 'full_address', 'filtered')\n","  countries = [\"India\", \"Hong Kong\", \"United States\", \"Canada\", \"United Kingdom\", \"Malaysia\", \"Singapore\", \"France\", \"Philippines\", \"Thailand\", \"Japan\", \"China\", \"Germany\", \"Russia\", \"Sweden\"]\n","  df = df.filter(df.lang == \"en\").filter(df.full_address.isin(countries))\n","#   df.groupBy('full_address').count().sort('count', ascending=False).show()\n","  df.write.format(\"csv\").save(\"wasbs://\"+month+\"@sentimentdatacs4225.blob.core.windows.net/\"+month+\"-\" + str(i) + \".csv\")\n","  print(\"completed: \"+ month + str(i))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"694a3177-f129-4780-852f-cb080ff57928"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"stopWordsDf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"value","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"created_at","nullable":true,"type":"date"},{"metadata":{},"name":"lang","nullable":true,"type":"string"},{"metadata":{},"name":"full_address","nullable":true,"type":"string"},{"metadata":{},"name":"filtered","nullable":false,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">completed: mar13\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">completed: mar13\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dataCleaning","dashboards":[],"language":"python","widgets":{},"notebookOrigID":919096953325539}},"nbformat":4,"nbformat_minor":0}